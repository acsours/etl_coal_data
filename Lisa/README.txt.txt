Group: Anna Sours, Anna Kantor, Andre Shearer, Lisa Caruana

ETL Project topic: Examination of International trends in coal electricity production, air pollution levels and health impacts by country (2000 - 2017)

Goal: Create a database that will allow users to examine connections between the number of coal power plants in a country (by megawatt hour), 
air pollution levels and recorded deaths from air pollution per 100,000 people between 2000 - 2017. 

Our group identified this as a gap in available databases. While there is an abundance of data available on each individual component, 
there is a lack of databases that combine data on electricity generated by coal power plants, air pollution levels and mortality rates from air pollution. 
We decided to use a relational database to allow comparisons between the data and for flexibility. 

Questions we were trying to answer include:
Is there a measurable change in health effects as regions move from coal to renewables? 
What countries are increasing electricity generation by coal fired power plants, decreasing their use and/or staying constant?
Are there any noticeable impacts on air pollution levels with changes in electricity generation by coal fired power plants?
Are there any noticeable changes in mortality rates from air pollution with changes in coal fired electricity production?

Data Sources:
Coal Power Generation 
New coal plants by country (Source: Global Energy Monitor)    
Retired coal plants by country (Source: Global Energy Monitor) 

Air Pollution Levels
Source: OECD Air Pollution Exposure databank 
OECD (2021), Air pollution exposure (indicator). doi: 10.1787/8d9dcc33-en (Accessed on 10 March 2021)
OECD (2021), Air pollution effects (indicator). doi: 10.1787/573e3faf-en (Accessed on 10 March 2021)
OECD (2021), Air and GHG emissions (indicator). doi: 10.1787/93d10cf7-en (Accessed on 10 March 2021) 
Mortality Rates from Air Pollution 
Air Pollution Exposure and Effects (Source: Kaggle) 
 
Unique Country Codes
Country Mapping - ISO, Continent, Region (Source: Kaggle)
Coal capacity data cleaning (Andre S): 
	
Extract: 
Identified the change in coal power generation year-over-year, by country. Change being defined as new capacity less retired capacity in megawatts (MW)
	

Transform: 
Data cleaning and shaping done in jupyter. See notebook "coal_metrics_transform.ipnyb" - for details. 
Issue: Some of the retired capacity is undated. 
Decision: After discussion with the team we reached consensus to assign the undated to 2020, as we know the power plants were retired by 2020.
Sub-issue: numeric columns come in as str (object) data type from CSV files, and source file had thousands separator as a comma.
Resolution: use looping to strip commas from columns that need to be numeric, and cast the results as int.
Issue: mis-matches in country names compared to lookup table. This is due to different formats (e.g., 'North Korea' vs. 'Korea, Republic of'), 
abbreviations in country names, and character set (diacritics in lookup table but not in coal capacity data. Scrubbed data frames for mis-match 
and manually corrected in Jupyter NB


Country Code Lookup Table (Anna Sours):

Extract: 
The raw datasets on air pollution, coal plants, and mortality rates did not contain standardized country identifiers. Some were listed by full name of country, 
some by the standardized ISO 3166-1 alpha-3 codes. To allow for joins by location in the database, a lookup table was deemed necessary. Using a csv from kaggle 
the country identifying information table was loaded into pandas as a dataframe for cleaning. The data frame contained several columns of information that were 
not used in the other datasets, so would not be useful identifiers on their own. Name, alpha-3 code, region, and sub-region were present in the other datasets 
used to create the database, so those columns were kept in addition to the numeric codes accompanying each (country, region, and sub-region).  Region and subregion 
duplicates are allowable in the table, since numerous countries may belong to one region or sub-region. Null values were not dropped because some countries 
may not have an identifiable region or sub-region, but the country alpha-3 code was still present and therefore useful as an identifier. Some of the country 
name values contained special characters such as an accent over a letter. In order to standardize across datasets, we removed the special characters from the 
lookup table and replaced them with the corresponding letter from the English alphabet. These values were chosen based on what was present in each dataset. 
Columns were renamed to use underscores and facilitate easier loading into SQL database. The data was then exported to a csv file to be loaded into the SQL database as a lookup table. 




Air Pollution Levels (Anna Kantor):
Data sources 
oecd_poll_exposure_mcgm3.csv - Data refer to population exposure to more than 10 micrograms/m3 of particulate matter and are expressed as annual averages.
oecd_poll_effect_mortality.csv – Data refer to number of premature deaths per 1 000 000 inhabitants attributable to ambient particulate matter
oecd_air_ghg_CO2_t_pc.csv – Data refers to gross direct emissions from fuel combustion only and data are provided by the International Energy Agency; measurement is tonnes per capita.
oecd_air_ghg_NOx_kg_pc.csv - emissions of nitrogen oxides (NOx)
oecd_air_ghg_SOx_kg_pc.csv – emissions of sulphur oxides (SOx) which are measured in kilograms per capita
The files were loaded to Jupyter Notebook using Pandas read_csv() method and converted to dataframes.
The created dataframes were checked for non-null values. The last column “Flag Codes”was dropped as a column with all null values.
The next step was to examine the context of the dataframes and only columns with potential interest such as “Location” (which represents country abbreviation code), “Time” (year), and “Value” were left.
“Location” column also had rows for groups of countries such as OECD, WLD, G20, etc. We created a list of those countries to remove the corresponding rows. 
These rows were removed using isin() “not in” equivalent method.

Merge:
The first two dataframes - pollution exposure to PM2.5 and mortality rate - were outer joined by pandas method merge() on both Location and Time columns. 
Suffixes were added to Value columns during the join. Outer join has been used in spite of creation Null values to keep all available data as it’s valuable.
Then the combined dataframe was merged with the next CO2 pollution dataframe using Left join on Location and Time columns. The Value column after the join was renamed.
The new combined dataframe was merged with NOx pollution dataframe and then with SOx pollution dataframe by using Left join on Location and Time columns. The corresponding suffixes were applied.
The columns “Location” and “Time” were renamed to “alpha_3” and “year” correspondingly.

Air Pollution Mortality Rates (Lisa Caruana):
The data came from a CSV file posted on Kaggle. It was loaded into Jupyter notebook using Pandas. Since the original file contained data on indoor air pollution deaths, 
this column and the total air pollution deaths were removed. The original file also contained around 1,000 rows of data summarizing mortality rates by region, 
sub-region and economic indicators. Since the Kaggle post and the file did not contain any information on what countries belonged to what groups, these rows were 
dropped by using .dropna on any rows that did not contain a three-letter country code. The columns labeled “Entity” and “Code” were changed to “Country” and “alpha_3” 
respectively. The data was then double-checked to see what form the information was in using .dtypes. Since each row was in the appropriate form (Country name and alpha_3 
as strings, year as integer, and mortality rates by float), no change was necessary. The data was then exported as a CSV file.  
Load: 

Our vision for practical use of this database involves a user making queries based on year or country to find data around air pollution, coal plants, and mortality 
rates for that time period or location. This would involve querying the database to create joins, and a structured relational database seemed to be the best fit. 
We chose to load these data sets into an SQL database. We also wanted standardization across the dataset, especially for country names that have a unique identifier. 
The use of foreign keys and primary keys available in a relational database was ideal for this purpose.

One table was created for each dataset: air pollution, coal plants, and mortality rates.  This kept the data in as similar to raw form as possible. We decided that 
any joins should take place in the SQL database, in order to make the database more efficient rather than performing joins on the data side. 

In the lookup table, all country identifiers were assigned a unique constraint. Alpha 3 code was standardized across datasets, but country names were found to 
have some special characters or formatting differences across datasets. While we accounted for this in the step of cleaning the lookup table, there is still a 
possibility that differences in country names exist, so decided not to assign country name as a foreign key in any tables to allow for more flexibility in the database. 


The only variable we chose to constrain as “not null” across all tables was the alpha-3 country code. We chose not to constrain null values in any of the other columns 
for the three primary tables because there were null values in some of the columns, due to missing datapoints for a certain year or metric. 


Pandas and SQLAlchemy were used to load the database. We first imported the csv files to dataframes using pandas, and renamed columns to match the 
SQL schema. We then made a connection into the postgres database and used the .to_sql method to load each of the dataframes into the database. 

During this process, we found an error due to foriegn key assignment - the air pollution csv alpha_3 column (which is a foreign key and depends 
on the country lookup table) contained a value that was not present in the country table. As such, we decided to drop that value from the dataframe at the time 
of loading to SQL. This was a prime example of the usefulness of foreign key constraints as a way to maintain data integrity. 


